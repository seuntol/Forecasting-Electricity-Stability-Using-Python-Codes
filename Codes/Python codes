``` py
#Load Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

#Load Dataset
dataset = pd.read_csv('./UCI_Dataset.csv')
dataset.head()

#Key Attributes
dataset.describe()

#Create Profile Report
 
#Importing package
import pandas_profiling as pp
from IPython.display import IFrame
 
# Profile Report
DiabetesReport = pp.ProfileReport(dataset)
DiabetesReport.to_file('UCI.html')
display(IFrame('UCI.html', width=900, height=350))

#Prepare for Models for Comparison

#Create x and y variables
x = dataset.drop('stabf', axis=1).to_numpy()
Y = dataset['stabf'].to_numpy()

#Load Library for Training
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,Y,test_size = 0.2,stratify=Y,random_state = 100)

# Use built-in isolation forest
from sklearn.ensemble import IsolationForest

# The prediction returns 1 if sample point is inlier. If outlier prediction returns -1
clf_all_features = IsolationForest(random_state=100)
clf_all_features.fit(x_train)

#Predict if a particular sample is an outlier using all features for higher dimensional data set.
y_pred_train = clf_all_features.predict(x_train)
y_pred_train2 =np.array(list(map(lambda x: x == 1, y_pred_train)))

# Exclude suggested outlier samples for improvement of prediction power/score
x_train_mod = x_train[y_pred_train2, ]
y_train_mod = y_train[y_pred_train2, ]

#Size of Datasets
print('Original Train Dataset Size : {}'.format(len(x_train)))
print('New Train Dataset Size      : {}'.format(len(x_train_mod)))

#Scale the Data
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train2 = sc.fit_transform(x_train_mod)
x_test2 = sc.fit_transform(x_test)

x_2 = sc.fit_transform(x)

#Models
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression

from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression

    
#Construct some pipelines 
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV

pipe_gnb = Pipeline([('scl', StandardScaler()),
                    ('clf', GaussianNB())])

#Construct some pipelines 
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

#Create Pipeline

pipeline =[]

pipe_logreg = Pipeline([('scl', StandardScaler()),
                    ('clf', LogisticRegression(solver='lbfgs',class_weight='balanced',
                                               random_state=100,max_iter=1000))])
pipeline.insert(0,pipe_logreg)

#Set grid search params 

modelpara =[]

param_gridlogreg = {'clf__C': [0.01, 0.1, 1, 10, 100], 
                    'clf__penalty': ['l2']}
modelpara.insert(0,param_gridlogreg)

#GNB (Note: GNB has no optimizing paramaters)
param_gridgnb = {}

#Optimize Models 

opt_gnb = GridSearchCV(estimator=pipe_gnb, param_grid=param_gridgnb,
                       cv=10, verbose=0,scoring='recall_weighted')

# List of pipelines for ease of iteration 
grids = [opt_gnb] 

# Dictionary of pipelines and classifier types for ease of reference 
grid_dict = {0:'gnb'}

#Define Plot for learning curve

from sklearn.model_selection import learning_curve

def plot_learning_curves(model):
    train_sizes, train_scores, test_scores = learning_curve(estimator=model,
                                                            X=x_train_mod, 
                                                            y=y_train_mod,
                                                            train_sizes= np.linspace(0.1, 1.0, 10),
                                                            cv=10,
                                                            scoring='recall_weighted',random_state=100)
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    test_mean = np.mean(test_scores, axis=1)
    test_std = np.std(test_scores, axis=1)
    
    plt.plot(train_sizes, train_mean,color='blue', marker='o', 
             markersize=5, label='training accuracy')
    plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std,
                     alpha=0.15, color='blue')

    plt.plot(train_sizes, test_mean, color='green', linestyle='--', marker='s', markersize=5,
             label='validation accuracy')
    plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std,
                     alpha=0.15, color='green')
    plt.grid()
    plt.xlabel('Number of training samples')
    plt.ylabel('Accuracy')
    plt.legend(loc='best')
    plt.ylim([0.5, 1.01])
    plt.show()

#Plot Learning Curve
print('Gaussian NB - Learning Curve')
plot_learning_curves(pipe_gnb)
print('\nLogistic Regression - Learning Curve')
plot_learning_curves(pipe_logreg)

#Model Analysis
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import cross_val_score

models=[]
models.append(('GaussianNB',pipe_gnb))
models.append(('Logistic Regression',pipe_logreg))

#Model Evaluation
results =[]
names=[]
scoring ='accuracy'
print('Model Evaluation - Accuracy Score')
for name, model in models:
    rkf=RepeatedKFold(n_splits=10, n_repeats=5, random_state=100)
    cv_results = cross_val_score(model,x_train_mod,y_train_mod,cv=rkf,scoring=scoring)
    results.append(cv_results)
    names.append(name)
    print('{} {:.2f} +/- {:.2f}'.format(name,cv_results.mean(),cv_results.std()))
print('\n')

fig = plt.figure(figsize=(5,5))
fig.suptitle('Boxplot View')
ax = fig.add_subplot(111)
sns.boxplot(data=results)
ax.set_xticklabels(names)
plt.ylabel('Accuracy')
plt.xlabel('Model')
plt.show()

#Define Gridsearch Function

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix  

def Gridsearch_cv(model, params):
    
    #Cross-validation Function
    cv2=RepeatedKFold(n_splits=10, n_repeats=5, random_state=100)
        
    #GridSearch CV
    gs_clf = GridSearchCV(model, params, cv=cv2,scoring='recall_weighted')
    gs_clf = gs_clf.fit(x_train_mod, y_train_mod)
    model = gs_clf.best_estimator_
    
    # Use best model and test data for final evaluation
    y_pred = model.predict(x_test2)
    #Identify Best Parameters to Optimize the Model
    bestpara=str(gs_clf.best_params_)
    
    #Output Heading
    print('\nOptimized Model')
    print('\nModel Name:',str(pipeline.named_steps['clf']))
        
#     #Feature Importance - optimized
#     print('Feature Importances')
#     for name, score in zip(list(dataset),gs_clf.best_estimator_.named_steps['clf'].feature_importances_):
#         print(name, round(score,2))
    
    #Output Validation Statistics
    print('\nBest Parameters:',bestpara)
    print('\n', confusion_matrix(y_test,y_pred))  
    print('\n',classification_report(y_test,y_pred))   
    
    #Transform the variables into binary (0,1) - ROC Curve
    from sklearn import preprocessing
    Forecast1=pd.DataFrame(y_pred)
    Outcome1=pd.DataFrame(y_test)
    lb1 = preprocessing.LabelBinarizer()
    OutcomeB1 =lb1.fit_transform(Outcome1)
    ForecastB1 = lb1.fit_transform(Forecast1)
    
    #Setup the ROC Curve
    from sklearn.metrics import roc_curve, auc
    from sklearn import metrics
    fpr, tpr, threshold = metrics.roc_curve(OutcomeB1, ForecastB1)
    roc_auc = metrics.auc(fpr, tpr)
    print('ROC Curve')
    #Plot the ROC Curve
    plt.title('Receiver Operating Characteristic')
    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
    plt.legend(loc = 'lower right')
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')
    plt.show()   

#Run Models

for pipeline, modelpara in zip(pipeline,modelpara):
    Gridsearch_cv(pipeline,modelpara)

from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV, RepeatedKFold
import matplotlib.pyplot as plt
from sklearn import preprocessing

# Assuming 'grids', 'x_train', 'y_train', 'x_test', 'y_test' are defined

# Create Confusion Matrix Table and Report
for idx, gs in enumerate(grids):
    
    # Fit grid search
    gs.fit(x_train, y_train)
    # Predict on test data with best params 
    y_pred2 = gs.predict(x_test) 
    
    # Create Dataframe to Compare Models
    target_names=['stable','unstable']
    print('\nEstimator: {}'.format(grid_dict[idx])) 
    print('\nBest Parameters:', str(gs.best_params_))
    print('\n', confusion_matrix(y_test, y_pred2))  
    print(classification_report(y_test, y_pred2, target_names=target_names))
    
    # Transform the variables into binary (0,1) - ROC Curve
    Forecast1 = pd.DataFrame(y_pred2)  # Fixed variable name from y_pred to y_pred2
    Outcome1 = pd.DataFrame(y_test)
    lb1 = preprocessing.LabelBinarizer()
    OutcomeB1 = lb1.fit_transform(Outcome1)
    ForecastB1 = lb1.fit_transform(Forecast1)

    # Setup the ROC Curve
    fpr, tpr, thresholds = roc_curve(OutcomeB1, ForecastB1)
    roc_auc = auc(fpr, tpr)

    # Plot the ROC Curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve for {grid_dict[idx]} Model')
    plt.legend(loc='lower right')
    plt.show()

from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import cross_validate
from sklearn.datasets import make_classification

# Generate some example data
x_train, y_train = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
x_2, Y = make_classification(n_samples=500, n_features=20, n_classes=2, random_state=42)

estimators = []

model1 = LogisticRegression(solver='lbfgs', class_weight='balanced', max_iter=1000, random_state=100)
estimators.append(('Logistic', model1))

voting_clf = VotingClassifier(estimators, voting='soft')

scoring = {'acc': 'accuracy', 'prec_macro': 'precision_macro', 'rec_macro': 'recall_macro'}
print('\nVoting Model')
for clf in (model1, voting_clf):
    clf.fit(x_train, y_train)  # Corrected this line
    ens_rkf1 = RepeatedKFold(n_splits=10, n_repeats=5, random_state=100)
    rKFcv = cross_validate(clf, x_2, Y, scoring=scoring, cv=ens_rkf1)
    print(clf.__class__.__name__, round(rKFcv['test_rec_macro'].mean(), 2))

from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import cross_validate
from sklearn.datasets import make_classification

# Generate some example data
x_train, y_train = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
x_2, Y = make_classification(n_samples=500, n_features=20, n_classes=2, random_state=42)

# Identify Models
lr = LogisticRegression(solver='lbfgs', class_weight='balanced', random_state=100)

estimators = []

mod1 = LogisticRegression(solver='lbfgs', class_weight='balanced', max_iter=1000, random_state=100)
estimators.append(('Logistic', mod1))

mod2 = AdaBoostClassifier(random_state=100)
estimators.append(('AdaBoost', mod2))

# Create Stacking Classifier
stackmod = StackingClassifier(estimators=estimators, final_estimator=lr)

scoring2 = {'acc': 'accuracy', 'prec_macro': 'precision_macro', 'rec_macro': 'recall_macro'}

print('\nStacking Model')
for clf in (mod1, mod2, stackmod):
    clf.fit(x_train, y_train)  # Corrected this line
    ens_rkf2 = RepeatedKFold(n_splits=10, n_repeats=5, random_state=100)
    rKFcv2 = cross_validate(clf, x_2, Y, scoring=scoring2, cv=ens_rkf2)
    print(clf.__class__.__name__, round(rKFcv2['test_rec_macro'].mean(), 2))

```
